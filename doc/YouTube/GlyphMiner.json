{"text": " In this video, we showcase a system for extracting glyphs from early typeset prints. The system helps for training OCR software by rapidly locating sample occurrences of given glyphs. Let's have a look at how this works. For this demonstration, we use scans of an incunable printed in Basel in 1497, containing a Latin translation of Sebastian Brandt's famous Narrenschiff. Here you see an overview of the documents that have been uploaded to our system. We select the incunable and go to the next screen. Now we see the title page. On the left, we can navigate to the other pages of this collection. Let's pick a page that contains a lot of text. In this example, we want to find occurrences of the character E to train an OCR engine on. First, we select a decent E somewhere in the text by selecting it with the rectangle tool. Note that we crop it generously to include the white space above and below. This white space distinguishes it from similar glyphs but with accents, such as the one to the right. Next, we tell the system that what we cropped was an E so that we can find it more easily later, and then we submit this as a new template. In the background, the system now starts a template matching algorithm that searches for occurrences over E on all of the pages in the collection. We have only uploaded 20 pages for this demonstration, but our algorithm could as well search all 320 pages of this incunable. This would just take a little longer. Once the template matching is done, we are presented with a set of candidate matches. They are color-coded according to their similarity to the template we indicated. As you can see, our system correctly located some Es. There are also a whole lot of false positives. At this point, we switch to the classification interface, which will help us to get rid of these false positives. Here, we are presented with a batch of nine candidates and have to tell the system which of them are correct and which are not. The system uses this information to iteratively train a logistic regression model from which it eventually learns to distinguish between the correct and the incorrect matches. For details on the algorithm involved, see the link in the description below. After about 10 iterations, the model is sufficiently trained, and we go back to the document view. Now, only those matches that the system classifies as correct are shown. We can see that almost all false positives have been removed, while most of the correct matches are still there. This is also true for the other pages in the collection. We have found thousands of occurrences of this glyphs in mere minutes. We now go to the glyph library, where the results from the previous step are aggregated. All detected Es are displayed in the list on the right, sorted by quality. About two-thirds of the way down the list, we see that our glyphs have started to show significant variance, but almost all of them are still correct. We can see that the glyphs have started to show significant variance, but almost all of them are still correct. All the way to the bottom, there are also some false positives, but relative to the number of glyphs found, this is not a significant fraction. Here is another glyph we prepared earlier using this process. We see much fewer occurrences, because the CT ligature is relatively rare. These data can now be downloaded as image files, or exported in the page.xml file. As an example of what you can do with the output of our system, we import the detected glyphs into FrankenPlus. This is software for generating synthetic draining data for the Tesseract OCR engine. It was developed at the Texas A&M University, and is designed to be used in a digitization pipeline for early prints. As you can see here, our results are similar to those of the previous step. This concludes our video, in which we gave a quick impression of the features of our system. We hope you enjoyed it, and say thanks for watching. Thank you for watching.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.0, "text": " In this video, we showcase a system for extracting glyphs from early typeset prints.", "tokens": [50364, 682, 341, 960, 11, 321, 20388, 257, 1185, 337, 49844, 22633, 950, 82, 490, 2440, 3467, 302, 22305, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12361765399421613, "compression_ratio": 1.492, "no_speech_prob": 0.07813301682472229}, {"id": 1, "seek": 0, "start": 8.0, "end": 14.0, "text": " The system helps for training OCR software by rapidly locating sample occurrences of given glyphs.", "tokens": [50764, 440, 1185, 3665, 337, 3097, 422, 18547, 4722, 538, 12910, 1628, 990, 6889, 5160, 38983, 295, 2212, 22633, 950, 82, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12361765399421613, "compression_ratio": 1.492, "no_speech_prob": 0.07813301682472229}, {"id": 2, "seek": 0, "start": 14.0, "end": 17.0, "text": " Let's have a look at how this works.", "tokens": [51064, 961, 311, 362, 257, 574, 412, 577, 341, 1985, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12361765399421613, "compression_ratio": 1.492, "no_speech_prob": 0.07813301682472229}, {"id": 3, "seek": 0, "start": 17.0, "end": 22.0, "text": " For this demonstration, we use scans of an incunable printed in Basel in 1497,", "tokens": [51214, 1171, 341, 16520, 11, 321, 764, 35116, 295, 364, 834, 409, 712, 13567, 294, 5859, 338, 294, 3499, 23247, 11, 51464], "temperature": 0.0, "avg_logprob": -0.12361765399421613, "compression_ratio": 1.492, "no_speech_prob": 0.07813301682472229}, {"id": 4, "seek": 0, "start": 22.0, "end": 29.0, "text": " containing a Latin translation of Sebastian Brandt's famous Narrenschiff.", "tokens": [51464, 19273, 257, 10803, 12853, 295, 31102, 11119, 83, 311, 4618, 13512, 1095, 6145, 3661, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12361765399421613, "compression_ratio": 1.492, "no_speech_prob": 0.07813301682472229}, {"id": 5, "seek": 2900, "start": 30.0, "end": 34.0, "text": " Here you see an overview of the documents that have been uploaded to our system.", "tokens": [50414, 1692, 291, 536, 364, 12492, 295, 264, 8512, 300, 362, 668, 17135, 281, 527, 1185, 13, 50614], "temperature": 0.0, "avg_logprob": -0.05151703381779218, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.0012845885939896107}, {"id": 6, "seek": 2900, "start": 34.0, "end": 38.0, "text": " We select the incunable and go to the next screen.", "tokens": [50614, 492, 3048, 264, 834, 409, 712, 293, 352, 281, 264, 958, 2568, 13, 50814], "temperature": 0.0, "avg_logprob": -0.05151703381779218, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.0012845885939896107}, {"id": 7, "seek": 2900, "start": 38.0, "end": 40.0, "text": " Now we see the title page.", "tokens": [50814, 823, 321, 536, 264, 4876, 3028, 13, 50914], "temperature": 0.0, "avg_logprob": -0.05151703381779218, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.0012845885939896107}, {"id": 8, "seek": 2900, "start": 40.0, "end": 44.0, "text": " On the left, we can navigate to the other pages of this collection.", "tokens": [50914, 1282, 264, 1411, 11, 321, 393, 12350, 281, 264, 661, 7183, 295, 341, 5765, 13, 51114], "temperature": 0.0, "avg_logprob": -0.05151703381779218, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.0012845885939896107}, {"id": 9, "seek": 2900, "start": 44.0, "end": 49.0, "text": " Let's pick a page that contains a lot of text.", "tokens": [51114, 961, 311, 1888, 257, 3028, 300, 8306, 257, 688, 295, 2487, 13, 51364], "temperature": 0.0, "avg_logprob": -0.05151703381779218, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.0012845885939896107}, {"id": 10, "seek": 2900, "start": 49.0, "end": 55.0, "text": " In this example, we want to find occurrences of the character E to train an OCR engine on.", "tokens": [51364, 682, 341, 1365, 11, 321, 528, 281, 915, 5160, 38983, 295, 264, 2517, 462, 281, 3847, 364, 422, 18547, 2848, 322, 13, 51664], "temperature": 0.0, "avg_logprob": -0.05151703381779218, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.0012845885939896107}, {"id": 11, "seek": 5500, "start": 55.0, "end": 61.0, "text": " First, we select a decent E somewhere in the text by selecting it with the rectangle tool.", "tokens": [50364, 2386, 11, 321, 3048, 257, 8681, 462, 4079, 294, 264, 2487, 538, 18182, 309, 365, 264, 21930, 2290, 13, 50664], "temperature": 0.0, "avg_logprob": -0.05664307862809561, "compression_ratio": 1.6859504132231404, "no_speech_prob": 0.0022626330610364676}, {"id": 12, "seek": 5500, "start": 61.0, "end": 65.0, "text": " Note that we crop it generously to include the white space above and below.", "tokens": [50664, 11633, 300, 321, 9086, 309, 48983, 281, 4090, 264, 2418, 1901, 3673, 293, 2507, 13, 50864], "temperature": 0.0, "avg_logprob": -0.05664307862809561, "compression_ratio": 1.6859504132231404, "no_speech_prob": 0.0022626330610364676}, {"id": 13, "seek": 5500, "start": 65.0, "end": 72.0, "text": " This white space distinguishes it from similar glyphs but with accents, such as the one to the right.", "tokens": [50864, 639, 2418, 1901, 11365, 16423, 309, 490, 2531, 22633, 950, 82, 457, 365, 35012, 11, 1270, 382, 264, 472, 281, 264, 558, 13, 51214], "temperature": 0.0, "avg_logprob": -0.05664307862809561, "compression_ratio": 1.6859504132231404, "no_speech_prob": 0.0022626330610364676}, {"id": 14, "seek": 5500, "start": 72.0, "end": 77.0, "text": " Next, we tell the system that what we cropped was an E so that we can find it more easily later,", "tokens": [51214, 3087, 11, 321, 980, 264, 1185, 300, 437, 321, 4848, 3320, 390, 364, 462, 370, 300, 321, 393, 915, 309, 544, 3612, 1780, 11, 51464], "temperature": 0.0, "avg_logprob": -0.05664307862809561, "compression_ratio": 1.6859504132231404, "no_speech_prob": 0.0022626330610364676}, {"id": 15, "seek": 5500, "start": 77.0, "end": 82.0, "text": " and then we submit this as a new template.", "tokens": [51464, 293, 550, 321, 10315, 341, 382, 257, 777, 12379, 13, 51714], "temperature": 0.0, "avg_logprob": -0.05664307862809561, "compression_ratio": 1.6859504132231404, "no_speech_prob": 0.0022626330610364676}, {"id": 16, "seek": 8200, "start": 82.0, "end": 85.0, "text": " In the background, the system now starts a template matching algorithm", "tokens": [50364, 682, 264, 3678, 11, 264, 1185, 586, 3719, 257, 12379, 14324, 9284, 50514], "temperature": 0.0, "avg_logprob": -0.05947278648294428, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.0003486330679152161}, {"id": 17, "seek": 8200, "start": 85.0, "end": 91.0, "text": " that searches for occurrences over E on all of the pages in the collection.", "tokens": [50514, 300, 26701, 337, 5160, 38983, 670, 462, 322, 439, 295, 264, 7183, 294, 264, 5765, 13, 50814], "temperature": 0.0, "avg_logprob": -0.05947278648294428, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.0003486330679152161}, {"id": 18, "seek": 8200, "start": 91.0, "end": 94.0, "text": " We have only uploaded 20 pages for this demonstration,", "tokens": [50814, 492, 362, 787, 17135, 945, 7183, 337, 341, 16520, 11, 50964], "temperature": 0.0, "avg_logprob": -0.05947278648294428, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.0003486330679152161}, {"id": 19, "seek": 8200, "start": 94.0, "end": 99.0, "text": " but our algorithm could as well search all 320 pages of this incunable.", "tokens": [50964, 457, 527, 9284, 727, 382, 731, 3164, 439, 42429, 7183, 295, 341, 834, 409, 712, 13, 51214], "temperature": 0.0, "avg_logprob": -0.05947278648294428, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.0003486330679152161}, {"id": 20, "seek": 8200, "start": 99.0, "end": 104.0, "text": " This would just take a little longer.", "tokens": [51214, 639, 576, 445, 747, 257, 707, 2854, 13, 51464], "temperature": 0.0, "avg_logprob": -0.05947278648294428, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.0003486330679152161}, {"id": 21, "seek": 8200, "start": 104.0, "end": 108.0, "text": " Once the template matching is done, we are presented with a set of candidate matches.", "tokens": [51464, 3443, 264, 12379, 14324, 307, 1096, 11, 321, 366, 8212, 365, 257, 992, 295, 11532, 10676, 13, 51664], "temperature": 0.0, "avg_logprob": -0.05947278648294428, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.0003486330679152161}, {"id": 22, "seek": 10800, "start": 108.0, "end": 114.0, "text": " They are color-coded according to their similarity to the template we indicated.", "tokens": [50364, 814, 366, 2017, 12, 66, 12340, 4650, 281, 641, 32194, 281, 264, 12379, 321, 16176, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07488711980672982, "compression_ratio": 1.6987951807228916, "no_speech_prob": 0.09316419810056686}, {"id": 23, "seek": 10800, "start": 114.0, "end": 118.0, "text": " As you can see, our system correctly located some Es.", "tokens": [50664, 1018, 291, 393, 536, 11, 527, 1185, 8944, 6870, 512, 2313, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07488711980672982, "compression_ratio": 1.6987951807228916, "no_speech_prob": 0.09316419810056686}, {"id": 24, "seek": 10800, "start": 118.0, "end": 122.0, "text": " There are also a whole lot of false positives.", "tokens": [50864, 821, 366, 611, 257, 1379, 688, 295, 7908, 35127, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07488711980672982, "compression_ratio": 1.6987951807228916, "no_speech_prob": 0.09316419810056686}, {"id": 25, "seek": 10800, "start": 122.0, "end": 125.0, "text": " At this point, we switch to the classification interface,", "tokens": [51064, 1711, 341, 935, 11, 321, 3679, 281, 264, 21538, 9226, 11, 51214], "temperature": 0.0, "avg_logprob": -0.07488711980672982, "compression_ratio": 1.6987951807228916, "no_speech_prob": 0.09316419810056686}, {"id": 26, "seek": 10800, "start": 125.0, "end": 130.0, "text": " which will help us to get rid of these false positives.", "tokens": [51214, 597, 486, 854, 505, 281, 483, 3973, 295, 613, 7908, 35127, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07488711980672982, "compression_ratio": 1.6987951807228916, "no_speech_prob": 0.09316419810056686}, {"id": 27, "seek": 10800, "start": 130.0, "end": 133.0, "text": " Here, we are presented with a batch of nine candidates", "tokens": [51464, 1692, 11, 321, 366, 8212, 365, 257, 15245, 295, 4949, 11255, 51614], "temperature": 0.0, "avg_logprob": -0.07488711980672982, "compression_ratio": 1.6987951807228916, "no_speech_prob": 0.09316419810056686}, {"id": 28, "seek": 10800, "start": 133.0, "end": 137.0, "text": " and have to tell the system which of them are correct and which are not.", "tokens": [51614, 293, 362, 281, 980, 264, 1185, 597, 295, 552, 366, 3006, 293, 597, 366, 406, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07488711980672982, "compression_ratio": 1.6987951807228916, "no_speech_prob": 0.09316419810056686}, {"id": 29, "seek": 13700, "start": 137.0, "end": 142.0, "text": " The system uses this information to iteratively train a logistic regression model", "tokens": [50364, 440, 1185, 4960, 341, 1589, 281, 17138, 19020, 3847, 257, 3565, 3142, 24590, 2316, 50614], "temperature": 0.0, "avg_logprob": -0.06213618596394857, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0024971929378807545}, {"id": 30, "seek": 13700, "start": 142.0, "end": 147.0, "text": " from which it eventually learns to distinguish between the correct and the incorrect matches.", "tokens": [50614, 490, 597, 309, 4728, 27152, 281, 20206, 1296, 264, 3006, 293, 264, 18424, 10676, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06213618596394857, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0024971929378807545}, {"id": 31, "seek": 13700, "start": 147.0, "end": 152.0, "text": " For details on the algorithm involved, see the link in the description below.", "tokens": [50864, 1171, 4365, 322, 264, 9284, 3288, 11, 536, 264, 2113, 294, 264, 3855, 2507, 13, 51114], "temperature": 0.0, "avg_logprob": -0.06213618596394857, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0024971929378807545}, {"id": 32, "seek": 13700, "start": 152.0, "end": 155.0, "text": " After about 10 iterations, the model is sufficiently trained,", "tokens": [51114, 2381, 466, 1266, 36540, 11, 264, 2316, 307, 31868, 8895, 11, 51264], "temperature": 0.0, "avg_logprob": -0.06213618596394857, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0024971929378807545}, {"id": 33, "seek": 13700, "start": 155.0, "end": 158.0, "text": " and we go back to the document view.", "tokens": [51264, 293, 321, 352, 646, 281, 264, 4166, 1910, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06213618596394857, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0024971929378807545}, {"id": 34, "seek": 16700, "start": 167.0, "end": 172.0, "text": " Now, only those matches that the system classifies as correct are shown.", "tokens": [50364, 823, 11, 787, 729, 10676, 300, 264, 1185, 1508, 11221, 382, 3006, 366, 4898, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1055897923258992, "compression_ratio": 1.6051282051282052, "no_speech_prob": 0.007865872234106064}, {"id": 35, "seek": 16700, "start": 172.0, "end": 176.0, "text": " We can see that almost all false positives have been removed,", "tokens": [50614, 492, 393, 536, 300, 1920, 439, 7908, 35127, 362, 668, 7261, 11, 50814], "temperature": 0.0, "avg_logprob": -0.1055897923258992, "compression_ratio": 1.6051282051282052, "no_speech_prob": 0.007865872234106064}, {"id": 36, "seek": 16700, "start": 176.0, "end": 179.0, "text": " while most of the correct matches are still there.", "tokens": [50814, 1339, 881, 295, 264, 3006, 10676, 366, 920, 456, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1055897923258992, "compression_ratio": 1.6051282051282052, "no_speech_prob": 0.007865872234106064}, {"id": 37, "seek": 16700, "start": 179.0, "end": 183.0, "text": " This is also true for the other pages in the collection.", "tokens": [50964, 639, 307, 611, 2074, 337, 264, 661, 7183, 294, 264, 5765, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1055897923258992, "compression_ratio": 1.6051282051282052, "no_speech_prob": 0.007865872234106064}, {"id": 38, "seek": 16700, "start": 183.0, "end": 188.0, "text": " We have found thousands of occurrences of this glyphs in mere minutes.", "tokens": [51164, 492, 362, 1352, 5383, 295, 5160, 38983, 295, 341, 22633, 950, 82, 294, 8401, 2077, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1055897923258992, "compression_ratio": 1.6051282051282052, "no_speech_prob": 0.007865872234106064}, {"id": 39, "seek": 18800, "start": 188.0, "end": 191.0, "text": " We now go to the glyph library,", "tokens": [50364, 492, 586, 352, 281, 264, 22633, 950, 6405, 11, 50514], "temperature": 0.0, "avg_logprob": -0.2113842282976423, "compression_ratio": 1.9813953488372094, "no_speech_prob": 0.10439246147871017}, {"id": 40, "seek": 18800, "start": 191.0, "end": 194.0, "text": " where the results from the previous step are aggregated.", "tokens": [50514, 689, 264, 3542, 490, 264, 3894, 1823, 366, 16743, 770, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2113842282976423, "compression_ratio": 1.9813953488372094, "no_speech_prob": 0.10439246147871017}, {"id": 41, "seek": 18800, "start": 194.0, "end": 200.0, "text": " All detected Es are displayed in the list on the right, sorted by quality.", "tokens": [50664, 1057, 21896, 2313, 366, 16372, 294, 264, 1329, 322, 264, 558, 11, 25462, 538, 3125, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2113842282976423, "compression_ratio": 1.9813953488372094, "no_speech_prob": 0.10439246147871017}, {"id": 42, "seek": 18800, "start": 200.0, "end": 203.0, "text": " About two-thirds of the way down the list,", "tokens": [50964, 7769, 732, 12, 38507, 295, 264, 636, 760, 264, 1329, 11, 51114], "temperature": 0.0, "avg_logprob": -0.2113842282976423, "compression_ratio": 1.9813953488372094, "no_speech_prob": 0.10439246147871017}, {"id": 43, "seek": 18800, "start": 203.0, "end": 206.0, "text": " we see that our glyphs have started to show significant variance,", "tokens": [51114, 321, 536, 300, 527, 22633, 950, 82, 362, 1409, 281, 855, 4776, 21977, 11, 51264], "temperature": 0.0, "avg_logprob": -0.2113842282976423, "compression_ratio": 1.9813953488372094, "no_speech_prob": 0.10439246147871017}, {"id": 44, "seek": 18800, "start": 206.0, "end": 209.0, "text": " but almost all of them are still correct.", "tokens": [51264, 457, 1920, 439, 295, 552, 366, 920, 3006, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2113842282976423, "compression_ratio": 1.9813953488372094, "no_speech_prob": 0.10439246147871017}, {"id": 45, "seek": 18800, "start": 209.0, "end": 212.0, "text": " We can see that the glyphs have started to show significant variance,", "tokens": [51414, 492, 393, 536, 300, 264, 22633, 950, 82, 362, 1409, 281, 855, 4776, 21977, 11, 51564], "temperature": 0.0, "avg_logprob": -0.2113842282976423, "compression_ratio": 1.9813953488372094, "no_speech_prob": 0.10439246147871017}, {"id": 46, "seek": 18800, "start": 212.0, "end": 215.0, "text": " but almost all of them are still correct.", "tokens": [51564, 457, 1920, 439, 295, 552, 366, 920, 3006, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2113842282976423, "compression_ratio": 1.9813953488372094, "no_speech_prob": 0.10439246147871017}, {"id": 47, "seek": 21500, "start": 215.0, "end": 219.0, "text": " All the way to the bottom, there are also some false positives,", "tokens": [50364, 1057, 264, 636, 281, 264, 2767, 11, 456, 366, 611, 512, 7908, 35127, 11, 50564], "temperature": 0.0, "avg_logprob": -0.12608238991270673, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.005009095184504986}, {"id": 48, "seek": 21500, "start": 219.0, "end": 225.0, "text": " but relative to the number of glyphs found, this is not a significant fraction.", "tokens": [50564, 457, 4972, 281, 264, 1230, 295, 22633, 950, 82, 1352, 11, 341, 307, 406, 257, 4776, 14135, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12608238991270673, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.005009095184504986}, {"id": 49, "seek": 21500, "start": 225.0, "end": 229.0, "text": " Here is another glyph we prepared earlier using this process.", "tokens": [50864, 1692, 307, 1071, 22633, 950, 321, 4927, 3071, 1228, 341, 1399, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12608238991270673, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.005009095184504986}, {"id": 50, "seek": 21500, "start": 229.0, "end": 235.0, "text": " We see much fewer occurrences, because the CT ligature is relatively rare.", "tokens": [51064, 492, 536, 709, 13366, 5160, 38983, 11, 570, 264, 19529, 11742, 1503, 307, 7226, 5892, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12608238991270673, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.005009095184504986}, {"id": 51, "seek": 21500, "start": 235.0, "end": 238.0, "text": " These data can now be downloaded as image files,", "tokens": [51364, 1981, 1412, 393, 586, 312, 21748, 382, 3256, 7098, 11, 51514], "temperature": 0.0, "avg_logprob": -0.12608238991270673, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.005009095184504986}, {"id": 52, "seek": 21500, "start": 238.0, "end": 241.0, "text": " or exported in the page.xml file.", "tokens": [51514, 420, 42055, 294, 264, 3028, 13, 87, 15480, 3991, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12608238991270673, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.005009095184504986}, {"id": 53, "seek": 24100, "start": 241.0, "end": 244.0, "text": " As an example of what you can do with the output of our system,", "tokens": [50364, 1018, 364, 1365, 295, 437, 291, 393, 360, 365, 264, 5598, 295, 527, 1185, 11, 50514], "temperature": 0.0, "avg_logprob": -0.18380807876586913, "compression_ratio": 1.5038461538461538, "no_speech_prob": 0.010649188421666622}, {"id": 54, "seek": 24100, "start": 244.0, "end": 248.0, "text": " we import the detected glyphs into FrankenPlus.", "tokens": [50514, 321, 974, 264, 21896, 22633, 950, 82, 666, 39678, 32111, 13, 50714], "temperature": 0.0, "avg_logprob": -0.18380807876586913, "compression_ratio": 1.5038461538461538, "no_speech_prob": 0.010649188421666622}, {"id": 55, "seek": 24100, "start": 248.0, "end": 253.0, "text": " This is software for generating synthetic draining data for the Tesseract OCR engine.", "tokens": [50714, 639, 307, 4722, 337, 17746, 23420, 42916, 1412, 337, 264, 314, 14239, 578, 422, 18547, 2848, 13, 50964], "temperature": 0.0, "avg_logprob": -0.18380807876586913, "compression_ratio": 1.5038461538461538, "no_speech_prob": 0.010649188421666622}, {"id": 56, "seek": 24100, "start": 253.0, "end": 256.0, "text": " It was developed at the Texas A&M University,", "tokens": [50964, 467, 390, 4743, 412, 264, 7885, 316, 5, 44, 3535, 11, 51114], "temperature": 0.0, "avg_logprob": -0.18380807876586913, "compression_ratio": 1.5038461538461538, "no_speech_prob": 0.010649188421666622}, {"id": 57, "seek": 24100, "start": 256.0, "end": 261.0, "text": " and is designed to be used in a digitization pipeline for early prints.", "tokens": [51114, 293, 307, 4761, 281, 312, 1143, 294, 257, 14293, 2144, 15517, 337, 2440, 22305, 13, 51364], "temperature": 0.0, "avg_logprob": -0.18380807876586913, "compression_ratio": 1.5038461538461538, "no_speech_prob": 0.010649188421666622}, {"id": 58, "seek": 24100, "start": 261.0, "end": 266.0, "text": " As you can see here, our results are similar to those of the previous step.", "tokens": [51364, 1018, 291, 393, 536, 510, 11, 527, 3542, 366, 2531, 281, 729, 295, 264, 3894, 1823, 13, 51614], "temperature": 0.0, "avg_logprob": -0.18380807876586913, "compression_ratio": 1.5038461538461538, "no_speech_prob": 0.010649188421666622}, {"id": 59, "seek": 26600, "start": 266.0, "end": 271.0, "text": " This concludes our video, in which we gave a quick impression of the features of our system.", "tokens": [50364, 639, 24643, 527, 960, 11, 294, 597, 321, 2729, 257, 1702, 9995, 295, 264, 4122, 295, 527, 1185, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12955325179629856, "compression_ratio": 1.2608695652173914, "no_speech_prob": 0.16228201985359192}, {"id": 60, "seek": 26600, "start": 271.0, "end": 274.0, "text": " We hope you enjoyed it, and say thanks for watching.", "tokens": [50614, 492, 1454, 291, 4626, 309, 11, 293, 584, 3231, 337, 1976, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12955325179629856, "compression_ratio": 1.2608695652173914, "no_speech_prob": 0.16228201985359192}, {"id": 61, "seek": 29600, "start": 296.0, "end": 298.0, "text": " Thank you for watching.", "tokens": [50364, 1044, 291, 337, 1976, 13, 50464], "temperature": 0.0, "avg_logprob": -0.6889171600341797, "compression_ratio": 0.7419354838709677, "no_speech_prob": 0.9921663999557495}], "language": "English"}